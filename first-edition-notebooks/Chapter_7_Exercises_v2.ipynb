{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 7 Exercises v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom-McDonald/studynotebooks/blob/master/Chapter_7_Exercises_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "E_Hs0XMxV2s0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Chapter 7 - Ensemble Learning and Random Forests\n",
        "---\n",
        "\n",
        "* **1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?**\n",
        "> Theoretically combining five models should result in higher precision, provided the models are indeed markedly different. If the models are sufficiently independent, each will make different types of errors, hence a combination of the five models will be more generalised and likely less prone to error than any one model alone. The models could be made more independent of one another by training each model on different subsets of the training data; this approach would likely result in better results from the ensemble than otherwise.\n",
        "\n",
        "* **2. What is the difference between hard and soft voting classifiers?**\n",
        "> A *hard voting classifier* predicts the class which is the most common prediction among all models in the ensemble (if two models vote '1' and one votes '0', the hard voting classifier predicts '1'). On the other hand, if the models within the ensemble support prediction probabilities (`predict_proba()`), *soft voting classifiers* can be used, which average the probability score for each class across all the models and predict the class with the highest overall probability. This method can often be preferable and give better results as it assigns higher weighting to more confident predictions.\n",
        "\n",
        "* **3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?**\n",
        "> *Bagging and pasting* (sampling subsets of the training data with and without replacement respectively) can be sped up in this way; multiple models are trained separately on subsets of the training data which lends itself well to being distributed across multiple CPUs/servers. *Random forests* are similarly straightforward to distribute as they consist of a number of decision trees which could all be trained on separate CPUs. However, *Boosting ensembles* cannot be scaled in such a way as boosting is an inherently sequential process where subsuquent models are trained with the weights of misclassified training instances increased, so the process cannot be distributed in order to speed it up. With *stacking ensembles*, all models in a given layer are independent of one another and can be trained on different servers, however the layers themselves are sequential so all models in one layer must have finished training before the following layer can compute.\n",
        "\n",
        "* **4. What is the benefit of out-of-bag evaluation?**\n",
        "> Out of bag evaluation tests models on data from the training set that was not part of the subset which that individual model itself was trained on. This is advantageous as it gives a good indication of how well a model generalises to unseen data without the need to put aside x% of the training set for validation, so more data is available for training the model, resulting in better performance.\n",
        "\n",
        "* **5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?**\n",
        "> Regular decision trees use a random subset of features when splitting each node and an ensemble of these trees makes up a *Random Forest*, however trees can be made even more randomised by setting random thresholds for features when splitting each node instead of searching for the best split like regular decision trees do. A forest of these extremely random trees is known as *Extremely Randomized Trees* or '*Extra-Trees*'. This considerably speeds up training as searching for the best splitting threshold at each node is the most time consuming part of training a forest. The extra randomness can help as such a model is highly unlikely to overfit a training set, so if a regular forest is overfitting, *Extra-Trees* may perform better.\n",
        "\n",
        "* **6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?**\n",
        "> Possible solutions to this issue would be increasing `n_estimators` to increase the size of the ensemble, extending the iterative process of adaptive boosting. Also increasing the learning rate may be beneficial as this will assign a higher weighting to misclassified instances, meaning they're more readily accounted for in subsuquent training cycles. Alternatively, decreasing regularization on the base estimator itself would likely solve this issue.\n",
        "\n",
        "* **7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?**\n",
        "> *Gradient Boosting* differs from *Adaptive Boosting* in that instead of reassigning higher weights to misclassified instances and retraining on the entire training set, the boosting process involves fitting new models to the residual errors from the previous step's model. In this scenario, the regularization technique of *shrinkage* is likely the best course of action, decreasing the learning rate but increasing the number of estimators in order to give predictions which generalise better (i.e. a model which doesn't overfit). However, it is possible to overfit by having too many estimators so early stopping would be a good strategy here in order to find the sweet spot.\n",
        "\n",
        "* **8. Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use the first 40,000 instances for training, the next 10,000 for validation, and the last 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?**\n",
        "\n",
        "> Again, the first step is loading in the MNIST data with the code used in previous exercises."
      ]
    },
    {
      "metadata": {
        "id": "-j3jnT9oV2s5",
        "colab_type": "code",
        "outputId": "5391f554-83f7-4782-8942-0ef9ea59f3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "np.random.seed(42)                                                               # makes notebook output reproducible across runs\n",
        "\n",
        "def sort_by_target(mnist):                                                       # function is required as 'fetch_openml returns' the unsorted MNIST data\n",
        "    reorder_train = np.array(sorted([(target, i) for i, target in \\\n",
        "                                     enumerate(mnist.target[:60000])]))[:, 1]    # 'sort_by_target' makes the dataset the same as is used in the book\n",
        "    reorder_test = np.array(sorted([(target, i) for i, target in  \\\n",
        "                                     enumerate(mnist.target[60000:])]))[:, 1]\n",
        "    mnist.data[:60000] = mnist.data[reorder_train]\n",
        "    mnist.target[:60000] = mnist.target[reorder_train]\n",
        "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
        "    mnist.target[60000:] = mnist.target[reorder_test + 60000]\n",
        "    \n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "mnist.target = mnist.target.astype(np.int8)                                      # fetch_openml() returns targets as strings\n",
        "sort_by_target(mnist)                                                            # fetch_openml() returns an unsorted dataset\n",
        "\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "print('Shape of MNIST data: ', X.shape)\n",
        "print('Shape of MNIST target data: ', y.shape)\n",
        "\n",
        "X_train_and_valid, y_train_and_valid = X[:60000], y[:60000]\n",
        "X_test, y_test = X[60000:], y[60000:]\n",
        "shuffle_index = np.random.permutation(60000)\n",
        "X_train_and_valid = X_train_and_valid[shuffle_index]\n",
        "y_train_and_valid = y_train_and_valid[shuffle_index]\n",
        "X_train = X_train_and_valid[:50000]\n",
        "y_train = y_train_and_valid[:50000]\n",
        "X_valid = X_train_and_valid[50000:60000]\n",
        "y_valid = y_train_and_valid[50000:60000]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of MNIST data:  (70000, 784)\n",
            "Shape of MNIST target data:  (70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HL7PTmkpXf_H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">The next step is training the classifiers; we'll train a Random Forest, Extra-Trees Classifier and a Logistic Regression Classifier and evaluate them using the validation set. LR was chosen over an SVC due to how poorly SVM models scale with large datasets (`LinearSVC` took 3-4 times as long to train for example)."
      ]
    },
    {
      "metadata": {
        "id": "DKyeqcY8CEJ9",
        "colab_type": "code",
        "outputId": "bd46fa90-7ece-4b71-991a-18af3648b03c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
        "etc = ExtraTreesClassifier(n_estimators=10, n_jobs=-1)\n",
        "lrc = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1,\n",
        "                        max_iter=20)\n",
        "models = [rfc, etc, lrc]\n",
        "model_names = ['Random Forest', 'Extra Trees', 'Logistic Regression']\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_valid)\n",
        "    print(model_names[i], 'accuracy on validation set:', \n",
        "          accuracy_score(y_valid, y_pred))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest accuracy on validation set: 0.9436\n",
            "Extra Trees accuracy on validation set: 0.9456\n",
            "Logistic Regression accuracy on validation set: 0.9073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3KToFhx3-ohm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> After evaluating those models on the validation set, we combine the three of them into an ensemble using a hard voting classifier. Hard voting classifiers are generally a better option than soft when the models are not highly optimised, and given that the model hyperparameters have not been tuned here, this is the approach we've taken here."
      ]
    },
    {
      "metadata": {
        "id": "kjP7uVV3CY39",
        "colab_type": "code",
        "outputId": "5a1b0978-623d-4cfd-a66b-a669c6a99a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "ensemble_clf = VotingClassifier([('rfc', rfc), ('etc', etc), ('lrc', lrc)],\n",
        "                               voting='hard', n_jobs=-1)\n",
        "ensemble_clf.fit(X_train, y_train)\n",
        "y_pred = ensemble_clf.predict(X_valid)\n",
        "print('Hard voting ensemble classifier accuracy on validation set:', \n",
        "     accuracy_score(y_valid, y_pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hard voting ensemble classifier accuracy on validation set: 0.9504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p3kEhXL1TVvQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> Finally, we use the models to make predictions on the test set. From the accuracy scores, we can see that the ensemble outperforms all three individual models on the test set, as well as on the validation set, as expected."
      ]
    },
    {
      "metadata": {
        "id": "sudh9uQuCeq1",
        "colab_type": "code",
        "outputId": "cb67a45d-1b83-4cf0-f9dd-8599dbc72b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "for i, model in enumerate(models):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(model_names[i], 'accuracy on test set:', \n",
        "          accuracy_score(y_test, y_pred))\n",
        "    \n",
        "y_pred = ensemble_clf.predict(X_test)\n",
        "print('Hard voting ensemble classifier accuracy on test set:', \n",
        "     accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest accuracy on test set: 0.9445\n",
            "Extra Trees accuracy on test set: 0.947\n",
            "Logistic Regression accuracy on test set: 0.9139\n",
            "Hard voting ensemble classifier accuracy on test set: 0.9528\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I8VeejR6V2tH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **9. Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?**\n",
        "\n",
        "> The first step is creating an array of the individual model predictions where each instance is a 3 element vector containing the predictions for that image from all three models. The target is the same value as previously."
      ]
    },
    {
      "metadata": {
        "id": "US1R_95DDVDG",
        "colab_type": "code",
        "outputId": "88e6044e-5aa5-418a-a02d-7147f922cf3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "valid_preds = [0, 0, 0]\n",
        "for i, model in enumerate(models):\n",
        "    y_pred = model.predict(X_valid)\n",
        "    valid_preds[i] = y_pred\n",
        "\n",
        "X_train = np.array(valid_preds).transpose()\n",
        "y_train = y_valid\n",
        "print(\"Training set shape:\", X_train.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set shape: (10000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rRdnt9CFiJxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "> We feed these predictions on the validation set into a Random Forest Classifier which acts as the blender, and then after transforming the individual model predictions on the test set into the same format as the training set, we can evaluate the blender accuracy on the test set. It gives an accuracy score higher than each of the individual models but not as high as the hard voting classifier from the previous exercise."
      ]
    },
    {
      "metadata": {
        "id": "YR6aH7IwHL04",
        "colab_type": "code",
        "outputId": "e11bf974-8fc0-4fc4-99fd-23e2374caf0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "blender = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "blender.fit(X_train, y_train)\n",
        "\n",
        "test_preds = [0, 0, 0]\n",
        "for i, model in enumerate(models):\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_preds[i] = y_pred\n",
        "\n",
        "X_test = np.array(test_preds).transpose()\n",
        "\n",
        "y_pred = blender.predict(X_test)\n",
        "print(\"Blender accuracy score:\", accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Blender accuracy score: 0.9518\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}