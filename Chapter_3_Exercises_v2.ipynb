{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 3 Exercises v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom-McDonald/studynotebooks/blob/master/Chapter_3_Exercises_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZyDFgFNvhI0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Chapter 3 - Classification\n",
        "---\n",
        "\n",
        "* **1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\n",
        "on the test set. Hint: the `KNeighborsClassifier` works quite well for this task;\n",
        "you just need to find good hyperparameter values (try a grid search on the\n",
        "`weights` and `n_neighbors` hyperparameters).**\n",
        "\n",
        "Firstly, we import the necessary libraries/functions and load the dataset directly from scikit-learn. Then we split the data into training and test sets (this is done already in the dataset; the first 60000 instance are training data and the last 10000 are test), and shuffle the training data so that there aren't too many instances of the same number together, which can affect model performance. "
      ]
    },
    {
      "metadata": {
        "id": "9fvonj0lg84v",
        "colab_type": "code",
        "outputId": "684a1a95-aaa8-485d-84f2-b4a39de50026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)                                                               # makes notebook output reproducible across runs\n",
        "\n",
        "def sort_by_target(mnist):                                                       # function is required as 'fetch_openml returns' the unsorted MNIST data\n",
        "    reorder_train = np.array(sorted([(target, i) for i, target in \\\n",
        "                                     enumerate(mnist.target[:60000])]))[:, 1]    # 'sort_by_target' makes the dataset the same as is used in the book\n",
        "    reorder_test = np.array(sorted([(target, i) for i, target in  \\\n",
        "                                     enumerate(mnist.target[60000:])]))[:, 1]\n",
        "    mnist.data[:60000] = mnist.data[reorder_train]\n",
        "    mnist.target[:60000] = mnist.target[reorder_train]\n",
        "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
        "    mnist.target[60000:] = mnist.target[reorder_test + 60000]\n",
        "    \n",
        "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
        "mnist.target = mnist.target.astype(np.int8)                                      # fetch_openml() returns targets as strings\n",
        "sort_by_target(mnist)                                                            # fetch_openml() returns an unsorted dataset\n",
        "\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "print('Shape of MNIST data: ', X.shape)\n",
        "print('Shape of MNIST target data: ', y.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
        "shuffle_index = np.random.permutation(60000)\n",
        "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of MNIST data:  (70000, 784)\n",
            "Shape of MNIST target data:  (70000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hMS5rNV_umNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we use the the `KNeigborsClassifier` to classify the digits. This would typically be done using `GridSearchCV`, but this takes hours to fit, so I completed the code to the exercise and then just looked up the `best_params_` in the exercise answers in order to save time. The grid search code is therefore block commented out."
      ]
    },
    {
      "metadata": {
        "id": "8mQ2CtLpumeP",
        "colab_type": "code",
        "outputId": "76fc7fe3-bca0-46d1-9ecf-880d18d4d5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# knn = KNeighborsClassifier(n_jobs=-1)\n",
        "\n",
        "# params = {\n",
        "#         'n_neighbors':[3, 4, 5],\n",
        "#         'weights':['uniform', 'distance']\n",
        "# }\n",
        "\n",
        "# grid_cv = GridSearchCV(knn, params, cv=3, verbose=18, n_jobs=-1)\n",
        "# grid_cv.fit(X_train, y_train)\n",
        "# y_pred = grid_cv.predict(X_train, y_train)\n",
        "# print(accuracy_score(y_train, y_pred))\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "knn_acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy score of KNN on test set is: ', knn_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of KNN on test set is:  0.9714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B32-zckp8_bI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **2. Write a function that can shift an MNIST image in any direction (left, right, up,\n",
        "or down) by one pixel.5 Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your\n",
        "best model on this expanded training set and measure its accuracy on the test set.\n",
        "You should observe that your model performs even better now! This technique of \n",
        "artificially growing the training set is called *data augmentation* or *training set expansion*. **"
      ]
    },
    {
      "metadata": {
        "id": "KLBExj2rm0ml",
        "colab_type": "code",
        "outputId": "9df8ab02-f991-4cd4-d23b-5ee3e9a497c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_digit(image):                                                           # function to plot digits, each consists of a 28x28 pixel array which we must shift\n",
        "    image = image.reshape((28, 28))\n",
        "    plt.figure(figsize=(2, 2))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "plot_digit(X_train[40000])\n",
        "print('The digit below has the target value of', y_train[40000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The digit below has the target value of 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI0AAACOCAYAAAAMyosLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABJhJREFUeJzt3UsobW8cxvHlErkr5ZJEMSMMlKGR\nZGRkQBlQZoRMlImJgSihjIUyFSlmBuRSLimJmShhJuVazmzn9/6z9nn+7bMvy/czWk9rW/t1znPe\n3rPWXmsnfX19fXmAIDnWA0DioTSQURrIKA1klAYySgMZpYGM0kBGaSCjNJBRGsgoDWSUBjJKAxml\ngYzSQEZpIKM0kFEayCgNZJQGMkoDGaWBjNJARmkgozSQURrIKA1klAYySgMZpYEsNdYDSDTv7++h\n7ba2NrNva2vL5IKCApOPj49NLisri/DoooOZBjJKAxmlgSyJZ+75Oz09NbmxsTG0/fHxYfYlJSX5\nHquoqMjkvb09k8vLy//PEKOOmQYySgMZpYGMNY3j6OjI5KamJpNfXl5C2+4fXUlJicn9/f0mX15e\nmvz09GTywsKCyTk5OeEHHAPMNJBRGsgoDWS//trT5OSkyePj4yZ/X8O4+vr6TB4bGzM5NzfX5M/P\nT5MPDw9N7uzsNHlpacnk/Pz8H8cSTcw0kFEayCgNZL/uPI27Rvl+LcnzPO/8/Nz35ycmJkLbPT09\nZl9GRobJ7hqltbXV5K6uLpPdv4qUlBST09PTfccWLcw0kFEayCgNZIE/T+OuYXp7e00Ot4Zxz+MM\nDg6GtpOT7b+5xcVFk9fW1nyz+94zMzO+Y4kXzDSQURrIKA1kgV/T3N7emryysuL7+u7ubpMHBgZM\n/r6Oubm5MftGRkaksV1fX0uvjxfMNJBRGsgoDWSBX9PMzc1Jr29paTHZvf7z3ejoqMn39/fSeyUq\nZhrIKA1kgftoxN3dncmVlZUmv729mVxdXW3y/v6+yZmZmSYPDQ2Ftt3T/uFuy3WlpaWZfHFxYXJF\nRYV0vGhhpoGM0kBGaSAL3H+5Dw4OTHbXMK719XWT/dYwnud5s7OzPx7LvY22pqbGZPfRIt8fxeZ5\n/73FJV4x00BGaSCjNJAFbk3T0NBgcrjTUPPz8ya753mWl5d//NmsrCyTz87OTHYvYezu7prsPlK2\nqqrKd6zxgpkGMkoDGaWBLHBrmtRU+yu5t8q+vr6aPDU15Xs893pSaWlpaHtnZ8fscx/5ur29bbJ7\ny0uiCsZvgaiiNJBRGsgCt6YpLi42eXV11eTh4WGTHx8ffY/X3t5u8vT0dGjb/Sjo8/Ozye5X9bjc\nr/ZJFMw0kFEayCgNZIH7jLDq4eHBd39hYeFfH8td0+Tl5fm+/uTkxOTa2tq/fq9YYqaBjNJARmkg\nC9x5GpWyZglnY2PDd7/7GWL3MfiJgpkGMkoDGaWB7NevaSLp6urKd39dXZ3J8XqvdjjMNJBRGsgo\nDWSsaaKovr4+1kOICGYayCgNZJQGMtY0UdTR0RHrIUQEMw1klAYySgMZpYGM0kBGaSD79bewRJL7\nzbzNzc0mZ2dnm7y5ufnPx/QvMNNARmkgozSQsaaBjJkGMkoDGaWBjNJARmkgozSQURrIKA1klAYy\nSgMZpYGM0kBGaSCjNJBRGsgoDWSUBjJKAxmlgYzSQEZpIKM0kFEayCgNZJQGMkoDGaWBjNJARmkg\nozSQURrI/gAXmfGPFoigwAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gN_zk2dA4W54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The function below takes an input of a string, 'up', 'down', 'left' or 'right' to shift each image in the training set one pixel in the given direction, creating four new datasets of 60,000 images each. The new augmented training set therefore is 300,000 (5*60,000) images long, with the corresponding number of target variables."
      ]
    },
    {
      "metadata": {
        "id": "kZQSSR7ZnmBo",
        "colab_type": "code",
        "outputId": "a96aa332-f417-4e18-bc76-01aca184d3ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "def shift_mnist_images(direction, X_train):\n",
        "    X_train_shift = np.copy(X_train)\n",
        "    if direction == 'up':\n",
        "        shift_vert = -1\n",
        "        shift_horiz = 0\n",
        "    elif direction == 'down':\n",
        "        shift_vert = 1\n",
        "        shift_horiz = 0\n",
        "    elif direction == 'left':\n",
        "        shift_vert = 0\n",
        "        shift_horiz = -1\n",
        "    elif direction == 'right':\n",
        "        shift_vert = 0\n",
        "        shift_horiz = 1\n",
        "    for i in range(len(X_train_shift)):\n",
        "        image = X_train_shift[i]\n",
        "        image = image.reshape((28,28))\n",
        "        image = np.roll(image, (shift_vert, shift_horiz), axis=(0,1))\n",
        "        image = image.reshape(784,)\n",
        "        X_train_shift[i] = image\n",
        "    return(X_train_shift)\n",
        "\n",
        "print(X_train.shape, 'is the shape of the original training set.')\n",
        "print(y_train.shape, 'is the shape of the original training targets.\\n')\n",
        "X_train_up = shift_mnist_images('up', X_train)\n",
        "X_train_down = shift_mnist_images('down', X_train)\n",
        "X_train_left = shift_mnist_images('left', X_train)\n",
        "X_train_right = shift_mnist_images('right', X_train)\n",
        "\n",
        "X_train_aug = np.concatenate([X_train, X_train_up, X_train_down, X_train_left,\n",
        "                             X_train_right])\n",
        "y_train_aug = np.concatenate([y_train, y_train, y_train, y_train, y_train])\n",
        "print(X_train_aug.shape, 'is the shape of the augmented training set.')\n",
        "print(y_train_aug.shape, 'is the shape of the augmented training targets.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784) is the shape of the original training set.\n",
            "(60000,) is the shape of the original training targets.\n",
            "\n",
            "(300000, 784) is the shape of the augmented training set.\n",
            "(300000,) is the shape of the augmented training targets.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KK9EURgj4uUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This augmented training data can now be fed back into the KNN model to improve the accuracy of the predictions. Augmenting the training set in this fashion has lead to a ~0.5% increase in accuracy."
      ]
    },
    {
      "metadata": {
        "id": "Jgm0Y5vv41GJ",
        "colab_type": "code",
        "outputId": "4ab5fd42-eb15-42aa-95f7-2e8e92d8ae58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\n",
        "knn.fit(X_train_aug, y_train_aug)\n",
        "y_pred = knn.predict(X_test)\n",
        "knn_acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy score of KNN on test set is: ', knn_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score of KNN on test set is:  0.9763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3vv_sFCJ5KLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **3. Tackle the Titanic dataset. A great place to start is on [Kaggle](https://www.kaggle.com/c/titanic). **\n",
        "\n",
        "Firstly I downloaded the data via the Kaggle API, instructions for using the API on Colab can be found [here.](https://colab.research.google.com/drive/1DofKEdQYaXmDWBzuResXWWvxhLgDeVyl#scrollTo=0hyJ4Rb5HMwH) "
      ]
    },
    {
      "metadata": {
        "id": "10uQVhNs5V6Z",
        "colab_type": "code",
        "outputId": "75efa1b9-83c6-4c36-80fe-18181180b03f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files                                                   # manually upload the kaggle.josn API key file, download from 'My Account' on Kaggle\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a39baaf9-ed7b-423e-84d8-145f9994762e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a39baaf9-ed7b-423e-84d8-145f9994762e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"tommcd\",\"key\":\"2fb26ed7071c6de5e2dd0164a3e0ef90\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "uiKeSEXUX__s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle                                                           # install Kaggle API client and move API key to correct path\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fev2pPRrYmlY",
        "colab_type": "code",
        "outputId": "57af2167-8235-49d0-efa5-acc67332ab65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c titanic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train.csv to /content\n",
            "\r  0% 0.00/59.8k [00:00<?, ?B/s]\n",
            "100% 59.8k/59.8k [00:00<00:00, 24.1MB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/28.0k [00:00<?, ?B/s]\n",
            "100% 28.0k/28.0k [00:00<00:00, 27.3MB/s]\n",
            "Downloading gender_submission.csv to /content\n",
            "  0% 0.00/3.18k [00:00<?, ?B/s]\n",
            "100% 3.18k/3.18k [00:00<00:00, 2.50MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MgmhZIxqY0I8",
        "colab_type": "code",
        "outputId": "ecdc3277-5b43-4e9d-880c-98f4934bcb6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install sklearn-pandas\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import scipy.stats as ss\n",
        "\n",
        "train = pd.read_csv('train.csv', header=0, index_col=0)\n",
        "test = pd.read_csv('test.csv', header=0, index_col=0)\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-pandas\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/48/4e1461d828baf41d609efaa720d20090ac6ec346b5daad3c88e243e2207e/sklearn_pandas-1.8.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.11.0->sklearn-pandas) (1.11.0)\n",
            "Installing collected packages: sklearn-pandas\n",
            "Successfully installed sklearn-pandas-1.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PassengerId</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Survived  Pclass  \\\n",
              "PassengerId                     \n",
              "1                   0       3   \n",
              "2                   1       1   \n",
              "3                   1       3   \n",
              "4                   1       1   \n",
              "5                   0       3   \n",
              "\n",
              "                                                          Name     Sex   Age  \\\n",
              "PassengerId                                                                    \n",
              "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
              "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
              "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
              "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
              "5                                     Allen, Mr. William Henry    male  35.0   \n",
              "\n",
              "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
              "PassengerId                                                          \n",
              "1                1      0         A/5 21171   7.2500   NaN        S  \n",
              "2                1      0          PC 17599  71.2833   C85        C  \n",
              "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "4                1      0            113803  53.1000  C123        S  \n",
              "5                0      0            373450   8.0500   NaN        S  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "rbPms-ZSZwP4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After downloading the data, we can separate the target variable and begin to preprocess the data before modelling."
      ]
    },
    {
      "metadata": {
        "id": "UnNsHoJNZ8UJ",
        "colab_type": "code",
        "outputId": "5920b946-bf45-4042-da1e-908277be43d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "y_train = train['Survived']\n",
        "X_train = train.drop('Survived', axis=1)\n",
        "print(X_train.info())\n",
        "print(X_train.describe())\n",
        "\n",
        "X_train = X_train.drop('Ticket', axis=1)                                         # dropping 'Ticket' fields \n",
        "X_test = test.drop('Ticket', axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 891 entries, 1 to 891\n",
            "Data columns (total 10 columns):\n",
            "Pclass      891 non-null int64\n",
            "Name        891 non-null object\n",
            "Sex         891 non-null object\n",
            "Age         714 non-null float64\n",
            "SibSp       891 non-null int64\n",
            "Parch       891 non-null int64\n",
            "Ticket      891 non-null object\n",
            "Fare        891 non-null float64\n",
            "Cabin       204 non-null object\n",
            "Embarked    889 non-null object\n",
            "dtypes: float64(2), int64(3), object(5)\n",
            "memory usage: 76.6+ KB\n",
            "None\n",
            "           Pclass         Age       SibSp       Parch        Fare\n",
            "count  891.000000  714.000000  891.000000  891.000000  891.000000\n",
            "mean     2.308642   29.699118    0.523008    0.381594   32.204208\n",
            "std      0.836071   14.526497    1.102743    0.806057   49.693429\n",
            "min      1.000000    0.420000    0.000000    0.000000    0.000000\n",
            "25%      2.000000   20.125000    0.000000    0.000000    7.910400\n",
            "50%      3.000000   28.000000    0.000000    0.000000   14.454200\n",
            "75%      3.000000   38.000000    1.000000    0.000000   31.000000\n",
            "max      3.000000   80.000000    8.000000    6.000000  512.329200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B6Tk1Q5Aa_0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From the above information, we can see that we'll likely need to impute some age values using `Imputer`. *Ticket*  has been dropped from the dataset as these unique identifiers likely won't help us much in classifying the passengers (in any non-complex fashion at least).\n",
        "\n",
        "*Embarked, Pclass, Parch, SibSp* and *Sex* must be one-hot encoded as these are categorical variables. For reasons explained [here on Stack Overflow](https://stackoverflow.com/questions/21057621/sklearn-labelencoder-with-never-seen-before-values), I used `pd.get_dummies` to do this as opposed to `OneHotEncoder` from `scikit-learn`, as the latter can't deal with unseen categorical values in the test set, whereas with `get_dummies` we can simply ignore this unseen variable.\n",
        "\n",
        "We can also create classes to do some basic feature engineering, here we'll use the *Cabin* column to determine whether a passenger has a cabin or not, and we'll create a new feature equal to the length of the *Name* field string."
      ]
    },
    {
      "metadata": {
        "id": "621nRjv-cOQR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class name_length_feature(BaseEstimator, TransformerMixin):                      # creating transformers for adding new features to the data\n",
        "    def __init__(self, add_name_length=True):                                    # replacing 'Name' with 'name_length' and 'Cabin' with 'has_cabin',\n",
        "        self.add_name_length = add_name_length\n",
        "    def fit(self, X):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X['name_length'] = X['Name'].apply(len)\n",
        "        return X.drop('Name', axis=1)\n",
        "\n",
        "class has_cabin_feature(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, add_has_cabin_feature=True):\n",
        "        self.add_has_cabin_feature = add_has_cabin_feature\n",
        "    def fit(self, X):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X['has_cabin'] = pd.notnull(X['Cabin'])\n",
        "        return X.drop('Cabin', axis=1)\n",
        "\n",
        "class categorical_encoding(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, encode=True):\n",
        "        self.encode = encode\n",
        "    def fit(self, X):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X_dummies = pd.get_dummies(pd.DataFrame(X))\n",
        "        return X_dummies\n",
        "\n",
        "num_pipeline = Pipeline([                                                        # creating separate pipelines for numerical and categorical features,\n",
        "        ('imputer', SimpleImputer(strategy='median')),                           # then combining them using ColumnTransformer\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', categorical_encoding())\n",
        "    ])\n",
        "\n",
        "num_attribs = ['Age', 'Fare', 'name_length']\n",
        "cat_attribs = ['Pclass', 'Sex', 'SibSp', 'Parch', 'has_cabin', 'Embarked']\n",
        "\n",
        "combined_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        ('cat', cat_pipeline, cat_attribs),\n",
        "    ])\n",
        "\n",
        "full_pipeline = Pipeline([                                                       # combining the ColumnTransformer pipeline with the feature engineering\n",
        "        ('add_has_cabin', has_cabin_feature()),\n",
        "        ('add_name_length', name_length_feature()),\n",
        "        ('num_cat_pipeline', combined_pipeline)\n",
        "    ])\n",
        "\n",
        "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
        "X_test_prepared = full_pipeline.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxd9pDvhmXdy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After putting the pipeline together to prepare the data, we can fit a Support Vector Classifier (SVC) to the training set and make some predictions on the test data, before submitting this to Kaggle."
      ]
    },
    {
      "metadata": {
        "id": "I65V_d4zbuFt",
        "colab_type": "code",
        "outputId": "cad0b48a-5fc0-43d9-f78f-d01101994f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "cell_type": "code",
      "source": [
        "svc = SVC(kernel='linear', C=100, gamma=0.001)\n",
        "svc.fit(X_train_prepared, y_train)\n",
        "y_pred = svc.predict(X_train_prepared)\n",
        "svc_score = accuracy_score(y_train, y_pred)\n",
        "print('Accuracy of predictions on training set:', svc_score)\n",
        "\n",
        "X_test_prepared = np.delete(X_test_prepared, 16, axis=1)                         # this deletion was necessary to remove two categorical instances of '9' in the test\n",
        "y_pred_test = svc.predict(X_test_prepared)                                       # data which were unseen in the training data, hence giving the arrays different dimensions\n",
        "submission = pd.DataFrame(y_pred_test, index=test.index)\n",
        "submission.columns = ['Survived']\n",
        "submission.to_csv('titanic_submission_28012019.csv')\n",
        "submission.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of predictions on training set: 0.8024691358024691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PassengerId</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Survived\n",
              "PassengerId          \n",
              "892                 0\n",
              "893                 1\n",
              "894                 0\n",
              "895                 0\n",
              "896                 1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "stCf4yrw0WMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This inital submission came out at **76.6% accuracy placing around 7,000th on the leaderboard**. Doing some cross validation and hyperparameter tuning should improve this by quite a bit."
      ]
    },
    {
      "metadata": {
        "id": "qrmB55w80lEw",
        "colab_type": "code",
        "outputId": "a43006eb-dc25-4077-a8f9-5e286e862d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "param_dist = {'n_estimators':ss.randint(50,1000), \n",
        "              'min_samples_split':ss.randint(2, 10), \n",
        "              'min_samples_leaf':ss.randint(1,4)}\n",
        "\n",
        "rand_cv = RandomizedSearchCV(rfc, param_distributions=param_dist, \n",
        "                                   n_iter=10, cv=5, \n",
        "                                   scoring='neg_mean_squared_error', \n",
        "                                   return_train_score=True, n_jobs=-1, \n",
        "                                   verbose=True)\n",
        "\n",
        "rand_cv.fit(X_train_prepared, y_train)\n",
        "y_pred = rand_cv.predict(X_train_prepared)\n",
        "rfc_score = accuracy_score(y_train, y_pred)\n",
        "print('Accuracy of predictions on training set:', rfc_score)\n",
        "y_pred_test = rand_cv.predict(X_test_prepared)                                       # data which were unseen in the training data, hence giving the arrays different dimensions\n",
        "submission = pd.DataFrame(y_pred_test, index=test.index)\n",
        "submission.columns = ['Survived']\n",
        "submission.to_csv('titanic_submission2_28012019.csv')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   33.1s\n",
            "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   35.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of predictions on training set: 0.936026936026936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HVUFW5N5OBut",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Cross validation via a randomized grid search yielded a slightly better result, with an **accuracy of 77% moving up the leaderboard by ~1000 places**.\n",
        "\n",
        "\n",
        "* **4. Build a spam classifier (a more challenging exercise):**\n",
        ">* Download examples of spam and ham from Apache SpamAssassin’s public datasets.\n",
        ">* Unzip the datasets and familiarize yourself with the data format.\n",
        ">* Split the datasets into a training set and a test set.\n",
        ">* Write a data preparation pipeline to convert each email into a feature vector.\n",
        "Your preparation pipeline should transform an email into a (sparse) vector\n",
        "indicating the presence or absence of each possible word. For example, if all\n",
        "emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email\n",
        "“Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1]\n",
        "(meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is\n",
        "present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\n",
        "each word.\n",
        ">* You may want to add hyperparameters to your preparation pipeline to control\n",
        "whether or not to strip off email headers, convert each email to lowercase,\n",
        "remove punctuation, replace all URLs with “URL,” replace all numbers with\n",
        "“NUMBER,” or even perform stemming (i.e., trim off word endings; there are\n",
        "Python libraries available to do this).\n",
        ">* Then try out several classifiers and see if you can build a great spam classifier,\n",
        "with both high recall and high precision."
      ]
    },
    {
      "metadata": {
        "id": "NW4IO_C9Pw9q",
        "colab_type": "code",
        "outputId": "150a8469-fa28-40ec-a795-b7cd29556474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2\",    # scraping the files from the Apache website\n",
        "    'spam.tar.bz2')\n",
        "urllib.request.urlretrieve (\n",
        "    \"https://spamassassin.apache.org/old/publiccorpus/20050311_spam_2.tar.bz2\",\n",
        "    'spam_2.tar.bz2')\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://spamassassin.apache.org/old/publiccorpus/20030228_hard_ham.tar.bz2\",\n",
        "    'hard_ham.tar.bz2')\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham.tar.bz2\",\n",
        "    'easy_ham.tar.bz2')\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://spamassassin.apache.org/old/publiccorpus/20030228_easy_ham_2.tar.bz2\",\n",
        "    'easy_ham_2.tar.bz2')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('easy_ham_2.tar.bz2', <http.client.HTTPMessage at 0x7fe367323780>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "OtixNVWftavc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spam = tarfile.open('spam.tar.bz2').extractall()                                 # opening all the .tar files so the individual emails are accessible\n",
        "spam_2 = tarfile.open('spam_2.tar.bz2').extractall()\n",
        "hard_ham = tarfile.open('hard_ham.tar.bz2').extractall()\n",
        "easy_ham = tarfile.open('easy_ham.tar.bz2').extractall()\n",
        "easy_ham_2 = tarfile.open('easy_ham_2.tar.bz2').extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tIQez_ZPw0-N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now the files are extracted and in our file system, we can get to work on reading the individual emails into Python."
      ]
    },
    {
      "metadata": {
        "id": "s8ArixqmvYqT",
        "colab_type": "code",
        "outputId": "746637dc-b0f6-444c-94f9-fc0aa0112784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def read_emails(parent_folder):\n",
        "    emails = []\n",
        "    for filename in os.listdir(parent_folder):\n",
        "        file_path = parent_folder + '/' + filename\n",
        "        file = open(file_path, 'r', encoding = \"ISO-8859-1\")\n",
        "        content = file.read()\n",
        "        emails.append(content)\n",
        "    return emails\n",
        "\n",
        "spam = read_emails('spam')\n",
        "spam_2 = read_emails('spam_2')\n",
        "easy_ham = read_emails('easy_ham')\n",
        "easy_ham_2 = read_emails('easy_ham_2')\n",
        "hard_ham = read_emails('hard_ham')\n",
        "\n",
        "all_spam = spam + spam_2\n",
        "all_ham = easy_ham + easy_ham_2 + hard_ham\n",
        "print('We now have', len(all_spam), 'spam emails and', len(all_ham), \n",
        "      'ham emails to use for training and evaluating a model.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We now have 1898 spam emails and 4153 ham emails to use for training and evaluating a model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OvC6Zt50znyC",
        "colab_type": "code",
        "outputId": "a3788c7f-23dd-4878-fb1c-f252ad627978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "all_emails = all_spam + all_ham\n",
        "target = (len(all_spam)*[1]) + (len(all_ham)*[0])\n",
        "df = pd.DataFrame({'is_spam': target, 'email': all_emails})\n",
        "df = df.sample(frac=1, random_state=123).reset_index(drop=True)                                    # this line shuffles the df randomly, keeping all of the rows (i.e. frac=1)\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=123)\n",
        "X_train, X_test = train['email'].tolist(), test['email'].tolist()\n",
        "y_train, y_test = train['is_spam'].tolist(), test['is_spam'].tolist()\n",
        "\n",
        "print(len(X_train), 'emails in training set.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4840 emails in training set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "45d4eo5L5J5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have shuffled lists containing all the emails (as strings) and the target variables classifying them as spam (1) or ham (0). Using `train_test_split` I've set aside 20% of the data as a test set and separated the target variable from the email in both sets, so now work on the preparation pipeline can begin.\n",
        "\n",
        "Using `CountVectorizer` from `sklearn`, we can create sparse vectors of how many occurences there are of every single word in every email. It turns out there are around 165,000 unique words in the training set and after these will effectively act as our features for training a classifier. It makes sense to just keep this data in sparse format as scikit-learn can deal with data like this; converting half a billion data points to a dense format would be memory intensive.\n",
        "\n",
        "I created four transformers to try out on the data, the first replaces all numbers in the text with the string 'NUMBER', the second does the same thing for URLs, the third strips all the superfluous HTML code away from the email and finally the fourth converts all text in the email to lowercase."
      ]
    },
    {
      "metadata": {
        "id": "f9-cPf3e3Oin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class replace_numbers(BaseEstimator, TransformerMixin):                          # creating transformers for adding new features to the data\n",
        "    def __init__(self, replace_numbers=True):                                    # 1) replacing numbers in text with 'NUMBER'\n",
        "        self.replace_numbers = replace_numbers\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X[:] = [re.sub(\"\\d+\", \" NUMBER \", email) for email in X]\n",
        "        return X\n",
        "    \n",
        "class urls_removed(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, urls_removed=True):                                       # 2) replacing all URLs in text with 'URL'\n",
        "        self.urls_removed = urls_removed\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X[:] = [re.sub(r'http\\S+', 'URL', email) for email in X]\n",
        "        return X\n",
        "    \n",
        "class strip_html(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strip_html=True):                                         # 3) strip HTML code away leaving just the text\n",
        "        self.strip_html = strip_html\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X[:] = [BeautifulSoup(email).get_text() for email in X]\n",
        "        return X\n",
        "    \n",
        "class lowercase(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lowercase=True):                                          # 4) convert email to lowercase\n",
        "        self.lowercase = lowercase\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X, y=None):\n",
        "        X[:] = [email.lower() for email in X]\n",
        "        return X\n",
        "\n",
        "text_pipe = Pipeline([\n",
        "        ('number_replacer', replace_numbers()),\n",
        "        ('url_remover', urls_removed()),\n",
        "        ('html_stripper', strip_html()),\n",
        "        ('lowercase', lowercase()),\n",
        "        ('vectorizer', CountVectorizer()),\n",
        "        ('lrc', LogisticRegression(random_state=123))\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "svCNzeGbAmQt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Having created the pipeline of transformers finished off with a classifier, we can perform cross validation and check that using these transformers is actually beneficial to the model. A Logistic Regression Classifier has been used here as it's a robust but easy to use algorithm which doesn't require scaling."
      ]
    },
    {
      "metadata": {
        "id": "x6LugPSkQDGn",
        "colab_type": "code",
        "outputId": "8e085ebb-d4ce-4c33-daf0-6b0b5070d08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "        'number_replacer__replace_numbers': [True, False],\n",
        "        'url_remover__urls_removed':[True, False],\n",
        "        'html_stripper__strip_html':[True, False],\n",
        "        'lowercase__lowercase':[True, False]\n",
        "}\n",
        "\n",
        "feature_search = GridSearchCV(text_pipe, param_grid, cv=3, verbose=True,\n",
        "                             n_jobs=-1)\n",
        "feature_search.fit(X_train, y_train)\n",
        "train_acc = accuracy_score(y_train, feature_search.predict(X_train))\n",
        "test_acc = accuracy_score(y_test, feature_search.predict(X_test))\n",
        "\n",
        "print(feature_search.best_params_)\n",
        "print(train_acc, 'is the accuracy of the classifier on the training set.')\n",
        "print(test_acc, 'is the accuracy of the classifier on the test set.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 12.9min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'html_stripper__strip_html': True, 'lowercase__lowercase': True, 'number_replacer__replace_numbers': True, 'url_remover__urls_removed': True}\n",
            "1.0 is the accuracy of the classifier on the training set.\n",
            "0.9851362510322048 is the accuracy of the classifier on the test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NW4QMJ6k_Uvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see from the Grid Search, the preprocessing steps all improve the model as they are all present in the final selection. The best model returned gives a 98.5% accuracy rate which is pretty good for a fairly basic pipeline. Improvements could be made by stemming words in the email, removing the header or pulling specific information from the header, such as the subject line.\n",
        "\n",
        "The precision and recall scores were similarly good, both around 97.5%.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0QJVotOALXeA",
        "colab_type": "code",
        "outputId": "b96b3878-4554-4259-a6dc-4faa0b2dfc41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test, feature_search.predict(X_test))\n",
        "recall = recall_score(y_test, feature_search.predict(X_test))\n",
        "\n",
        "print('Precision score on test set:', precision)\n",
        "print('Recall score on test set:', recall)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision score on test set: 0.9745042492917847\n",
            "Recall score on test set: 0.9745042492917847\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}