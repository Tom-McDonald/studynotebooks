{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4 Exercises v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom-McDonald/studynotebooks/blob/master/Chapter_4_Exercises_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0ykM1BrkISMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chapter 4 - Training Models\n",
        "---\n",
        "\n",
        "* **1. What Linear Regression training algorithm can you use if you have a training set with millions of features?**\n",
        "> Gradient Descent training algorithms (e.g. Batch, Stochastic, Mini-Batch) are all well suited to training a Linear Regression model with a large number of features, whereas using the Normal Equation doesn't scale well and is best suited to data with fewer features. \n",
        "\n",
        "* **2. Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?**\n",
        "> Gradient Descent algorithms require scaled data as they perform very poorly otherwise, either taking an extremely long time to reach the optimal model parameters or not doing so at all. This eventuality can be avoided by scaling the features in the data during preprocessing (e.g. by using a scaling function from scikit-learn) so they all fall within a similar range of values.\n",
        "\n",
        "* **3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?**\n",
        "> No, as the cost function for Logistic Regression (the log-loss function) is convex and therefore only has a global minimum, which a Gradient Descent algorithm will always find given enough training time and learning rate that's not too large.\n",
        "\n",
        "* **4. Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?**\n",
        "> No, they do not. Batch Gradient Descent will descend smoothly towards the optimal parameters due to the fact it samples the entire training set at every iteration, whereas Stochastic GD and Mini-Batch sample a single random instance and a random batch of instances respectively, and due to this randomness, these algorithms don't settle at the global minimum like Batch GD does, they 'jump' around the minimum, never truly converging.\n",
        "\n",
        "* **5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?**\n",
        "> If the training error is not increasing, then overfitting is likely the problem which can be possibly solved by decreasing model complexity or feeding more training data into the model. Alternatively, if the training error is also increasing then it's more likely the case that the learning rate is too high and the model is not able to converge on the optimal solution, reducing the learning rate to decrease the GD step size will likely help in this situation.\n",
        "\n",
        "* **6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?**\n",
        "> No, it is not. This is because Mini-batch GD uses random samples on each iteration, so unlike Batch gradient descent, an increased validation error doesn't necessarily mean that the algorithm has reached some form of minimum. Mini-batch GD will 'jump' around due to randomness involved in the process, so it's best to set the algorithm to stop when the validation error hasn't decreased in a certain number of epochs (10, for example), at which point the model should revert to the parameters which returned the lowest error.\n",
        "\n",
        "* **7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?**\n",
        "> Stochastic GD is likely to reach the vicinity of the optimal solution fastest as it only uses one training instance per iteration, making it a much quicker option than either of the Batch GD algorithms. However, only Batch GD will actually converge fully as it uses the entire training set at each iteration. Stochastic and Mini-batch GD can be coerced into converging via stimulated annealing, which is the process of gradually reducing the learning rate during the training process, the idea being that the algorithm will take smaller and smaller steps until it converges on the global minimum.\n",
        "\n",
        "* **8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?**\n",
        "> If the validation error is much higher than the training error, the model is likely overfitting the training set. If possible, exposing the model to more data by increasing the size of the training set will likely help the situation. Reducing model complexity by either selecting a simpler algorithm with fewer polynomial degrees or introducing regularization (Ridge or Lasso for example) to constrain the model parameters should also combat this issue.\n",
        "\n",
        "* **9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter Î± or reduce it?**\n",
        "> Both errors being similarly high likely means the model is underfitting the data; this corresponds to high bias, as the model is not complex enough to give satisfactory predictions. The regularization hyperparameter should be decreased in this instance to lessen the constraints on the model and allow a better fit to the training set.\n",
        "\n",
        "* **10. Why would you want to use:** \n",
        "  * **Ridge Regression instead of Linear Regression?**\n",
        "  > Ridge is a good default algorithm to use instead of simple Linear Regression as introducing regularization, even if it's to a small degree, is generally always a good idea as it constrains the model from becoming too complex and overfitting the training data.\n",
        "  * **Lasso instead of Ridge Regression?**\n",
        "  > Lasso automatically discards unimportant features, so would be the perfect choice if you had data with a large number of features, many of which you believe to be superfluous, as Lasso will just ignore them and consider the important predictors.\n",
        "  * **Elastic Net instead of Lasso?**\n",
        "  > Elastic Net is a weighted combination of Lasso and Ridge (governed by a fraction, `r`) and is typically preferred to Lasso as Lasso can behave unpredictably when there are more features than training instances or when strong correlation exists between features.\n",
        "\n",
        "* **11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?**\n",
        "> This is a multioutput problem, as there are two different classes for each of the two outputs, therefore this is suited to using two Logistic Regression classifiers, not a multinomial/Softmax Regression classifier which is used for multiclass problems.\n",
        "\n",
        "* **12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn).**\n",
        "> Batch Gradient Descent means that we'll be using the entire training set at every iteration, and early stopping means that once the validation error increases instead of decreasing, we'll stop the training process. Softmax regression is effectively just multiclass Logistic Regression."
      ]
    },
    {
      "metadata": {
        "id": "lbux1vuLIMnJ",
        "colab_type": "code",
        "outputId": "c44df260-356e-4989-b0f8-4495519c23aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "np.random.seed(42)\n",
        "\n",
        "iris = datasets.load_iris()                                                      # loading in all 150 training instances; \n",
        "X = iris['data'][:,(2,3)]                                                        # there are two features and three classes of flower\n",
        "y = iris['target']\n",
        "\n",
        "X_biased = np.c_[np.ones([len(X), 1]), X]                                        # adding bias term of 1 to every instance,\n",
        "                                                                                 # as required for logistic regression\n",
        "print('Shape of data:', X_biased.shape)\n",
        "print('Shape of target:', y.shape)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data: (150, 3)\n",
            "Shape of target: (150,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mTzpDOOnUguH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we need to split the data into a training and test set without using scikit-learn. This is done by shuffling the indices of the data, selecting the training set size (80%) and then indexing the data using the shuffled indices, thereby randomly selecting a test set from the data."
      ]
    },
    {
      "metadata": {
        "id": "yZ1US-lkfxTj",
        "colab_type": "code",
        "outputId": "fb30735b-7e45-4de4-b89d-1574f03f7fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "indices_shuffled = np.random.permutation(len(y))\n",
        "train_size = int(len(y)*0.8)\n",
        "train_indices = indices_shuffled[:train_size]\n",
        "test_indices = indices_shuffled[train_size:]\n",
        "X_train, X_test = X_biased[train_indices], X_biased[test_indices]\n",
        "y_train, y_test = y[train_indices], y[test_indices]\n",
        "print('Shape of training set:', X_train.shape)\n",
        "print('Shape of test set:', X_test.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training set: (120, 3)\n",
            "Shape of test set: (30, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uBnXtbd3jJ69",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step is one-hot encoding the target variables, as currently due to the fact there's three classes (0, 1 and 2), the target values are not binary. This is done in a generalised fashion by finding the number of samples and classes/categories in the target array and using these values to create a one-hot encoded array where all the elements are binary."
      ]
    },
    {
      "metadata": {
        "id": "HQNpxfA9jY1s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def onehot(target):\n",
        "    n_categories = np.max(target) + 1\n",
        "    n_samples = len(target)\n",
        "    onehot_array = np.zeros((n_samples, n_categories), dtype=np.int64)\n",
        "    onehot_array[np.arange(n_samples), target] = 1\n",
        "    return onehot_array\n",
        "\n",
        "y_test_enc = onehot(y_test)\n",
        "y_train_enc = onehot(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZYtMH6jDPNw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we've prepared the data, we can define the softmax regression function we'll use for batch gradient descent. This is given by:"
      ]
    },
    {
      "metadata": {
        "id": "AUJJFri0Au_r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "    class_exps = np.exp(logits)\n",
        "    sum_exps = np.sum(class_exps, axis=1, keepdims=True)\n",
        "    return class_exps/sum_exps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "As95AVuT28Yr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we can start the training process. "
      ]
    },
    {
      "metadata": {
        "id": "2ZTPneFq3AWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d2c8bf79-b055-44f4-ec7b-556eb06f309c"
      },
      "cell_type": "code",
      "source": [
        "n_inputs = X_train.shape[1]                                                      # number of features\n",
        "n_outputs = len(np.unique(y_train))                                              # number of categories/flowers\n",
        "n_epochs = 10001\n",
        "epsilon = 1e-7\n",
        "learning_rate = 0.25\n",
        "m = len(X_train)                                                                 # number of training samples\n",
        "theta = np.random.randn(n_inputs, n_outputs)                                     # random initialisation of theta\n",
        "lowest_loss = np.inf\n",
        "final_epoch = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    logits = X_train.dot(theta)\n",
        "    y_pred_proba = softmax(logits)\n",
        "    loss = - np.mean(np.sum(y_train_enc * np.log(y_pred_proba + epsilon), \n",
        "                           axis=1))\n",
        "    error = y_pred_proba - y_train_enc\n",
        "    if epoch % 1000 == 0:\n",
        "        print('Epoch:', epoch, '. Loss:', loss)\n",
        "    gradients = (1/m) * X_train.T.dot(error)\n",
        "    theta = theta - (learning_rate * gradients)\n",
        "    if loss < lowest_loss:\n",
        "        lowest_loss = loss\n",
        "    if loss > lowest_loss:\n",
        "        lowest_loss = loss\n",
        "        final_epoch = epoch\n",
        "        print('Early stopping after epoch', final_epoch, '. Loss:', \n",
        "              lowest_loss)\n",
        "        break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 . Loss: 1.4689056020671027\n",
            "Early stopping after epoch 37 . Loss: 0.5308707297458729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4rfe8Jg83v08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The learning rate and number of epochs above were chosen specifically to display a situation in which early stopping occurs. It works as intended and stops training as the loss function begins to increase rather than decrease; a more sophisticated implementation would maybe allow a few iterations before stopping just in case the algorithm begins to decrease the loss once again."
      ]
    },
    {
      "metadata": {
        "id": "E1riAr_93dVs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2c74727-1803-4ad9-c443-223f2c4b1121"
      },
      "cell_type": "code",
      "source": [
        "logits = X_test.dot(theta)\n",
        "y_proba = softmax(logits)\n",
        "y_pred = np.argmax(y_proba, axis=1)\n",
        "accuracy = np.mean(y_pred == np.argmax(y_test_enc, axis=1))\n",
        "print('Accuracy on test set:', accuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PZaaOZcFKN7G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this case the accuracy is fairly poor but these parameters were the only ones I could choose which resulted in early stopping occuring (without having millions of iterations). Reducing the learning rate would improve accuracy considerably."
      ]
    }
  ]
}